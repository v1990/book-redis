# 原理

## 数据分布算法

- 节点取余分区
分区公式：`hash(key)%N`,但是当节点数量N调整时，数据节点的映射关系需要重新计算，会导致数据的重新迁移
- 一致性hash分区
    - 实现思路：为系统中的每个节点分配一个token，这些token构成一个哈希环，先根据key计算hash值，然后顺时针找到第一个大于等于该哈希值的token节点
    - 缺点：
        - 加减节点会造成哈希环中部分数据无法命中，需要手动处理或者忽略这部分数据，因此一致性哈希双方常用于缓存场景
        - 当使用少量节点时，节点变化将大范围影响哈希环中的数据映射，因此这种方式不适合少量数据节点的分布式方案
        - 普通的一致性哈希分区在增减节点时需要增加一倍或较少一半节点才能保证数据和负载的均衡
- 虚拟槽分区
    - 基于hash分区，使用分散度良好的哈希函数将所有数据映射到一个固定范围的整数集合中，整数定义为槽（slot）；
    - 这个范围一般远远大于节点数；
    - 槽是集群内数据管理和迁移的基本单位，采用大范围槽的主要目的是为了方便数据拆分和集群扩展；
    - 每个节点会负责一定数量的槽
   
## redis 分区算法  
redis cluser 采用虚拟槽分区，计算公式:`slot=CRC16(key)&16383`

特点：
- 解耦数据与节点之间的关系，简化了节点扩容和收缩的难度
- 节点自身维护槽的映射关系，不需要客户端或者代理服务维护槽分区的元数据
- 支持节点，槽，键之间的映射查询，用于数据路由，在线伸缩灯场景

## 集群功能的限制
- key批量操作：如mset,mget；目前只支持相同slot值的key执行批量操作
- key事务操作：同理只支持相同slot的多个key在同一节点上的事务操作
- key作为数据分区的最小粒度，因此不能将一个大的对象（如hash，list）等映射到不同的节点上
- 不支持多数据库空间：集群模式下只能使用一个数据空间，即 db 0；
- 复制结构只支持一层，从节点只能复制主节点，不支持嵌套树状复制结构


## hash tag
如果key中包含'{}',则只对大括号中的tag进行hash；
如`key="user:{UID}:xxx"`,只会对`UID`部分进行hash计算